# Stage 2: Final image
FROM {{ .BaseImage }}

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PATH=/opt/venv/bin:$PATH \
    {{- if .InstallOllama }}
    OLLAMA_HOST=127.0.0.1 \
    OLLAMA_PORT={{ .BackendPort }} \
    {{- end }}
    BACKEND_PORT={{ .BackendPort }}

# Install base dependencies
RUN apt-get update && \
    apt-get install -y \
        zstd \
        python3 \
        python3-venv \
        python3-pip \
        curl \
        wget \
        ca-certificates \
        supervisor \
        bash \
        rsync && \
    rm -rf /var/lib/apt/lists/*

# Install kdeps via official install script
RUN curl -LsSf https://raw.githubusercontent.com/kdeps/kdeps/main/install.sh | sh -s -- -b /usr/local/bin

{{- if .OSPackages }}

# Install OS packages
RUN apt-get update && \
    apt-get install -y{{ range .OSPackages }} {{ . }}{{ end }} && \
    rm -rf /var/lib/apt/lists/*
{{- end }}

# Install LLM backend
{{ .BackendInstall }}

# Install uv for Python package management
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv
RUN chmod +x /usr/local/bin/uv

# Create virtual environment
RUN uv venv /opt/venv

{{- if .PythonPackages }}

# Install Python packages
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install{{ range .PythonPackages }} {{ . }}{{ end }}
{{- end }}

{{- if .RequirementsFile }}

# Install from requirements file
COPY {{ .RequirementsFile }} /tmp/requirements.txt
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install -r /tmp/requirements.txt
{{- end }}

# Copy workflow files
COPY workflow.yaml /app/workflow.yaml
{{- if .HasResources }}
COPY resources/ /app/resources/
{{- end }}
{{- if .HasData }}
COPY data/ /app/data/
{{- end }}

# Copy entrypoint and supervisor config
COPY entrypoint.sh /entrypoint.sh
COPY supervisord.conf /etc/supervisord.conf
RUN chmod +x /entrypoint.sh

{{- if and .OfflineMode .Models .InstallOllama }}

# Download models during build time for offline mode
RUN mkdir -p /models

# Download Ollama models with cache mount to speed up subsequent builds
RUN --mount=type=cache,target=/root/.ollama \
    (OLLAMA_HOST=127.0.0.1 ollama serve &) && \
    sleep 10 && \
    {{- range $i, $model := .Models }}{{ if $i }} && \{{ end }}
    OLLAMA_HOST=127.0.0.1 ollama pull {{ $model }}{{ end }} && \
    pkill -f ollama && \
    sleep 2 && \
    # Copy from cache to the image layer
    cp -r /root/.ollama/* /models/
{{- end }}

WORKDIR /app

# Expose ports
EXPOSE {{ .APIPort }} {{ .BackendPort }}

# Use entrypoint for backend management
ENTRYPOINT ["/entrypoint.sh"]
CMD ["supervisord", "-c", "/etc/supervisord.conf"]
