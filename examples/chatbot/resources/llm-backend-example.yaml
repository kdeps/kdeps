apiVersion: kdeps.io/v1
kind: Resource

metadata:
  actionId: llmWithBackend
  name: LLM Chat with Backend Selection

run:
  restrictToHttpMethods: [POST]
  restrictToRoutes: [/api/v1/chat-backend]

  preflightCheck:
    validations:
      - get('q') != ''
    error:
      code: 400
      message: Query parameter 'q' is required

  chat:
    model: llama3.2:1b
    backend: ollama  # Local backend: ollama. Online providers: openai, anthropic, google, cohere, mistral, together, perplexity, groq, deepseek
    baseUrl: http://localhost:11434  # Optional: defaults to backend-specific default
    # For online providers, specify apiKey:
    # apiKey: "{{ env('OPENAI_API_KEY') }}"
    contextLength: 8192  # Context length in tokens: 4096, 8192, 16384, 32768, 65536, 131072, 262144 (default: 4096)
    role: user
    prompt: "{{ get('q') }}"
    scenario:
      - role: assistant
        prompt: You are a helpful AI assistant.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
    jsonResponse: true
    jsonResponseKeys:
      - answer
    timeoutDuration: 60s
