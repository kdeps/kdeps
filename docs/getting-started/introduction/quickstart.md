---
outline: deep
---

# Quickstart Guide

To get started, make sure the Kdeps CLI is already [installed](./installation.md) on your system.

In this quickstart, we'll guide you through creating a simple AI agent that demonstrates core Kdeps functionality. We will:

1. **Enable API mode and set up a basic API route**
2. **Configure an open-source LLM (Language Model) to process request data**
3. **Return structured JSON responses**

This tutorial will walk you through building a foundational AI agent that you can expand to handle more advanced workflows. Let's dive in!

## Generate the Configuration

Kdeps requires a `.kdeps.pkl` configuration file to function properly. This file is automatically generated the first time you run `kdeps`, with default settings including GPU mode (`dockerGPU`) set to `cpu`. Other supported `dockerGPU` values include `nvidia` and `amd`.

To create the configuration file, simply execute:

```bash
kdeps
```

After generation, an editor (defined by the `$EDITOR` environment variable) will open the configuration file, allowing you to change the `dockerGPU` setting to match your GPU hardware.

<img alt="Kdeps - Configuration" src="/configuration.gif" />

This is a one-time setup step. For more information, see the [configuration guide](../configuration/configuration.md).

## Create Your First AI Agent

Kdeps can generate a starter project using the `new` command. To create a new project, use:

```bash
kdeps new aiagentx
```

This command will:

1. **Create a project named `aiagentx`**
2. **Set up a project directory called `aiagentx`** with the following structure:
   - **`workflow.pkl`**: A [Workflow](../configuration/workflow.md) configuration file that defines the workflow and settings for your AI agent project
   - **`resources/`**: A directory pre-filled with example [`.pkl`](https://pkl-lang.org/index.html) resource files, including:
     - **`resources/python.pkl`**: A [Python Resource](../../core-resources/python.md) for running Python scripts
     - **`resources/response.pkl`**: A [Response Resource](../../core-resources/response.md) for preparing JSON responses from APIs
     - **`resources/client.pkl`**: An [HTTP Client Resource](../../core-resources/client.md) for making API requests
     - **`resources/exec.pkl`**: An [Exec Resource](../../core-resources/exec.md) for executing shell commands
     - **`resources/llm.pkl`**: An [LLM Resource](../../core-resources/llm.md) for interacting with language models
   - **`data/`**: A [Data](../../data-memory/data.md) directory for storing project-specific data

Once the setup is complete, you're ready to start building and customizing your AI agent.

> **Note:**
> In this quickstart guide, we will focus on configuring the `workflow.pkl` file and two key resources: `resources/chat.pkl` and `resources/response.pkl`.

## Configure the Workflow

The `workflow.pkl` file is the core configuration for your AI agent. It allows you to define API routes, configure custom Ubuntu repositories and packages, manage Anaconda and Python dependencies, and include LLM models. For comprehensive details, see the [Workflow](../configuration/workflow.md) documentation.

### Default Action

The `workflow.pkl` file defines the workflow and settings for your AI agent. Within this file, you'll find the `TargetActionID` configuration:

```apl
TargetActionID = "responseResource"
```

Here, `responseResource` refers to the ID of the target resource file, located in `resources/response.pkl`:

```apl
ActionID = "responseResource"
```

This resource will be executed as the default action whenever the AI agent runs.

### API Mode

The `workflow.pkl` file allows you to configure the AI agent to operate in API mode. Below is the generated configuration:

```apl
APIServerMode = true
APIServer {
    HostIP = "127.0.0.1"
    PortNum = 3000
    Routes {
        new {
            Path = "/api/v1/whois"
            Method = "GET" // Primary method for data retrieval
            // For both GET and POST support, define separate routes
        }
    }
}
```

This configuration creates an API server running on port `3000` with a route at `/api/v1/whois`. You can define multiple routes as needed.

With these settings, you can interact with the API using `curl` or similar tools. For example:

```bash
curl 'http://localhost:3000/api/v1/whois' -X GET
```

If you set `APIServerMode` to `false`, the AI agent will bypass the API server and directly execute the default action, exiting upon completion.

### LLM Models

The `workflow.pkl` file defines the LLM models to be included in the Docker image. Here's an example configuration:

```apl
AgentSettings {
    Models {
        "tinydolphin"
        // "llama3.2"
        // "llama3.1"
    }
    OllamaVersion = "0.8.0"
}
```

In this setup, the `llama*` models are commented out. To enable them, simply uncomment the corresponding lines.

Kdeps uses [Ollama](https://ollama.com) as its LLM backend. You can define as many Ollama-compatible models as needed to fit your use case.

For a comprehensive list of available Ollama-compatible models, visit the [Ollama model library](https://ollama.com/library).

> **Note: Additional Settings for Ubuntu, Python, and Anaconda Packages**
>
> You can also configure custom Ubuntu packages, repositories, and PPAs, along with additional Python or Anaconda packages. However, for this quickstart guide, we won't be using these settings.

## Configuring Resources

Once the `workflow.pkl` is configured, we can move on to setting up the resources. The AI agent will utilize two resources: `resources/chat.pkl` and `resources/response.pkl`.

Each resource is assigned a unique ID, which we use to reference the corresponding resource throughout the workflow.

### Resource Dependencies

Each resource contains a `Requires` section that defines the dependencies needed for that resource. The generated `resources/response.pkl` resource depends on the `chatResource` resource. Additionally, you can include other resources by uncommenting the relevant lines.

```apl
Requires {
    "chatResource"
    // "pythonResource"
    // "shellResource"
    // "httpResource"
}
```

When resource IDs are specified, they form a dependency graph that determines the execution order of the workflow. To learn more about how this graph-based dependency system works, refer to the [Graph Dependencies](../../workflow-control/kartographer.md) documentation.

### `resources/response.pkl` Resource

Within the `resources/response.pkl`, you'll find the following structure:

```apl
APIResponse {
    Success = true
    Response {
        Data {
            "@(llm.response("chatResource"))"
            // "@(python.stdout("pythonResource"))"
            // "@(exec.stdout("shellResource"))"
            // "@(client.responseBody("httpResource"))"
        }
    }
    Errors {
        new {
            Code = 0
            Message = ""
        }
    }
}
```

The `APIResponse` directive is the structure that will be converted into a JSON response.

The resulting JSON will look like this:

```json
{
    "success": true,
    "response": {
        "data": []
    },
    "errors": [{
        "code": 0,
        "message": ""
    }]
}
```

### Functions

Within the `data` JSON array, you will encounter references such as `llm`, `python`, `exec`, and `client`. These represent resource functions, which are fully customizable using [Apple PKL](https://pkl-lang.org). This flexibility allows you to extend and adapt the resources to meet your specific requirements. See the [Functions](../../functions-utilities/functions.md) documentation for more information.

Each resource corresponds to a specific function, as illustrated below:

```apl
llm.response("ID")
// python.stdout("ID")
// exec.stdout("ID")
// client.responseBody("ID")
```

In this AI agent workflow, the LLM response is retrieved from the `chatResource` and appended to the `data` JSON array.

### Resource Promise

Notice that each resource function is enclosed within `"@()"`. This follows the Kdeps convention, which ensures the resource is executed at a later stage. For more details on this convention, refer to the documentation on the [Promise Operations](../../workflow-control/promise.md) directive.

When invoking a resource function, always wrap it in `"@()"` along with double quotes, as in `"@(llm.response("chatResource"))"`. Depending on the output of this promise, you may sometimes need to escape it.

For example:

```apl
local clientResponse =
"""
@(client.responseBody("ID"))
"""
```

### `resources/chat.pkl` Resource

The chat resource is an `llm` resource that creates our LLM chat sessions.

If we look at the pkl file, we notice that the `Requires` section is empty. This is because `chatResource` does not depend on other resources to function.

```apl
Chat {
    Model = "llama3.1"
    Prompt = "Who is @(request.data())?"
    JSONResponse = true
    JSONResponseKeys {
        "first_name"
        "last_name"
        "parents"
        "address"
        "famous_quotes"
        "known_for"
    }
    TimeoutDuration = 60.s
}
```

The `Model` we use here is the same model that we define in the `workflow.pkl`. If you want to use multiple LLMs, you need to create additional `llm` resource files to use different defined LLM models.

In the `Prompt`, we use the function `@(request.data())`, which inserts the request data into the prompt. Referring back to the route configuration, the `curl` command can send request data using the `-d` flag, as shown:

```bash
curl 'http://localhost:3000/api/v1/whois' -X GET -d "Neil Armstrong"
```

Additionally, we have set `JSONResponse` to `true`, enabling the use of `JSONResponseKeys`. To ensure the output conforms to specific data types, you can define the keys with their corresponding types. For example: `first_name__string`, `famous_quotes__array`, `details__string`, or `age__integer`.

> **Important:**
> To accomplish defining the corresponding data types to keys, you'll need to adjust your LLM model, as the default `tinydolphin` model is not equipped to handle this. It is recommended to use models from the `llama3.*` family instead.

## Packaging

After finalizing our AI agent, we can proceed to package it. Packaged AI agents are single files that end with a `.kdeps` extension. With a single file, we can distribute, reuse, sell, and remix it in your AI agents.

To package an AI agent, simply run the `package` command specifying the folder:

```bash
kdeps package aiagentx
```

This will create the `aiagentx-1.0.0.kdeps` file in the current directory. We can now proceed to build the Docker image and container for this AI agent.

## Dockerization

After building the `kdeps` file, we can now run either `build` or `run` on the AI agent file.

The `build` command will create a Docker image, and the `run` command will both create a Docker image and container.

In this example, we will use `run` to do both:

```bash
kdeps run aiagentx-1.0.0.kdeps
```

This step will build the image, install the necessary packages, and download the Ollama models.

## Testing the AI Agent API

After creating the container, the LLM models will be downloaded by Ollama. This might take some time, and the API routes will not be available until Ollama has completed downloading the models.

After the models have been downloaded, we can proceed to make an API call using `curl`:

```bash
curl 'http://localhost:3000/api/v1/whois' -X GET -d "Neil Armstrong"
```

Expected response:

```json
{
  "errors": [
    {
      "code": 0,
      "message": ""
    }
  ],
  "response": {
    "data": [
      {
        "address": "Lebanon, Ohio, USA (birthplace)",
        "famous_quotes": [
          "That's one small step for man, one giant leap for mankind."
        ],
        "first_name": "Neil",
        "known_for": [
          "First person to walk on the Moon during the Apollo 11 mission",
          "Pioneering astronaut and naval aviator"
        ],
        "last_name": "Armstrong",
        "parents": {
          "father": "Stephen Koenig Armstrong",
          "mother": "Viola Louise Engel"
        }
      }
    ]
  },
  "success": true
}
```

<img alt="Kdeps - API" src="/api.gif" />

And that's it! We have created our first Kdeps AI Agent. Now, let's dive deeper into Kdeps configuration and learn more about workflows and resources.

## Next Steps

Now that you have a working AI agent, you can:

1. **[Explore Core Resources](../../core-resources/README.md)** to understand all available building blocks
2. **[Learn about Workflow Control](../../workflow-control/README.md)** to create more complex logic
3. **[Check out Advanced Resources](../../advanced-resources/README.md)** for specialized capabilities
4. **[Follow the Tutorials](../../tutorials/README.md)** for practical, real-world examples

Continue your journey with Kdeps by exploring these resources and building more sophisticated AI agents!